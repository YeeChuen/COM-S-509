{"cells":[{"cell_type":"code","metadata":{"source_hash":"ee12bcb4","execution_start":1700604400604,"execution_millis":5360,"deepnote_to_be_reexecuted":false,"cell_id":"2459083f9fc64ba99786430d6963e5a1","deepnote_cell_type":"code"},"source":"import csv\nimport numpy as np \nimport matplotlib.pyplot as plt\nimport os\nimport itertools\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot\nimport numpy.linalg \nimport numpy.random\nfrom sklearn.utils import shuffle\n\n\n\n#data parsing from csv to samples and labels\n#called by specific parsing methods such as \"parseAlzheimers\"\n# filename - name of file\n#returns list of samples X, list of labels y\ndef parseData(filename):\n    csv_data = pd.read_csv(filename)\n    numpy_data = csv_data.values\n    rows, columns = numpy_data.shape\n    X = numpy_data[:, :columns - 1]\n    y = numpy_data[:, columns - 1:]\n    X = np.array(X)\n    y = np.array(y)\n    return X, y\n\n\n#parsing method for specific file for readability\n# filename - name of the file\n#returns list of samples X, and list of labels y\ndef parseAlzheimers(filename):\n    X, y = parseData(\"handwriting_alzheimers.csv\")\n    X = X[:, 1:]\n    y = np.where(y == \"P\", 1, y)\n    y = np.where(y == \"H\", -1, y)\n\n    return X, y\n\n\n#splits given data based on the partitions given\n# X - samples\n# y - labels\n# trainSplit - float from 0.0 to 1.0 dictating % of training data\n# valSplit - float from 0.0 to 1.0, same as trainSplit but for validation data\n# testSplit - float from 0.0 to 1.0, same as trainSplit but for testing data\n#returns samples and labels for training, validation and testing\ndef splitData(X, y, trainSplit, valSplit, testSplit):\n    trainStop = int(trainSplit * X.shape[0])\n    valStop = int((trainSplit + valSplit) * X.shape[0])\n    train_x = X[0:trainStop, :]\n    train_y = y[0:trainStop]\n    val_x = X[trainStop:valStop, :]\n    val_y = y[trainStop:valStop]\n    test_x = X[valStop:, :]\n    test_y = y[valStop:]\n    return train_x, train_y, val_x, val_y, test_x, test_y\n\n\n#data partition method to split data for different weak models in bagging\n#this is sampling with replacement\n# tX - training samples\n# tY - training labels\n# splits - number of models that will be used in bagging\n# spp - samples per partition; int\n#returns list of lists of samples and labels for training and validation\ndef bagging_split(tX, tY, splits, spp):\n    tXs = []\n    tYs = []\n\n    size = tX.shape[0]\n\n    for i in range(splits):\n        tx = []\n        ty = []\n        \n        for j in range(spp):\n            index = np.random.randint(size)\n            tx.append(tX[index])\n            ty.append(tY[index])\n            \n        tx = np.array(tx)\n        ty = np.array(ty)\n        tXs.append(tx)\n        tYs.append(ty)\n\n    return tXs, tYs\n\n\n#data partition for boosting\n#sampling with replacement\n# tX - training samples\n# tY - training labels\n# spp - samples per partition; int\n#returns list of samples, list of labels, and list of index locations of samples\ndef boosting_split(tX, tY, spp):\n    tx = []\n    ty = []\n    ti = []\n\n    size = tX.shape[0]\n\n    for i in range(spp):\n        index = np.random.randint(size)\n        tx.append(tX[index])\n        ty.append(tY[index])\n        ti.append(index)\n\n    tx = np.array(tx)\n    ty = np.array(ty)\n    ti = np.array(ti)\n\n    return tx, ty, ti\n\n\n#normalization method over a range of 1\n# X - samples to be normalized\n#returns normalized samples X\ndef normalize(X):\n    rangeX = np.zeros(X.shape[1])\n    minX = np.zeros(X.shape[1])\n    normX = np.zeros(X.shape)\n\n    for i in range(X.shape[1]):\n        minX[i] = min(X[:, i])\n        rangeX[i] = max(X[:, i]) - minX[i]\n    for i in range(X.shape[0]):\n        for j in range(X.shape[1]):\n            normX[i][j] = (X[i][j] - minX[j]) / rangeX[j]\n\n    return normX\n\n\n#loss function for SVM; hinge-loss\n# C - coefficient factor for regularizing; weights if boosting\n# w - weights for the model\n# b - bias for the model\n# X - samples\n# y - labels\n#returns a list of losses\ndef hingeLoss(C, w, b, X, y):\n    reg_term = 0.5 * (w.T @ w)\n    losses = np.zeros(X.shape[0])\n\n    noboost = False\n    if (isinstance(C, int) or isinstance(C, float)):\n        noboost = True\n\n    for i in range(X.shape[0]):\n        opt_term = y[i] * ((w.T @ X[i]) + b)\n        if (noboost == True):\n            losses[i] = reg_term + C * max(0, 1 - opt_term)\n        else:\n            losses[i] = reg_term + C[i] * max(0, 1 - opt_term)\n    \n    return losses\n\n\n#training method for SVM model\n# C - coefficient factor used in loss function\n# X - training samples\n# y - training labels\n# batchSize - hyperparameter\n# learningRate - hyperparameter\n# epochs - hyperparameter\n#returns the model consisting of weights, bias, and a list of losses\ndef fit(C, X, y, batchSize, learningRate, epochs):\n    w = np.zeros(X.shape[1])\n    b = 0\n    lossList = []\n\n    for i in range(epochs):\n        losses = hingeLoss(C, w, b, X, y)\n        lossList.append(losses)\n        for batch in range(0, X.shape[0], batchSize):\n            batchX = X[batch:batch+batchSize, :]\n            batchy = y[batch:batch+batchSize, :]\n            wGradient, bGradient = gradient(C, batchX, batchy, w, b)\n            w = w - learningRate * wGradient\n            b = b - learningRate * bGradient\n    return w, b, lossList\n\n\n#computes the gradient of a particular iteration\n# C - coefficient for regularization; weights if boosting\n# X - training samples\n# y - training labels\n# w - current weights\n# b - current bias\n#returns the gradient for weights and bias separately\ndef gradient(C, X, y, w, b):\n    wGradient = 0\n    bGradient = 0\n\n    noboost = False\n    if (isinstance(C, int) or isinstance(C, float)):\n        noboost = True\n\n    for i in range(X.shape[0]):\n        dist = y[i] * (w.T @ X[i] + b)\n\n        if (dist < 1):\n            if (noboost == True):\n                wGradient += -1 * (C * y[i] * X[i])\n                bGradient += -1 * (C * y[i])\n            else:\n                wGradient += -1 * (C[i] * y[i] * X[i])\n                bGradient += -1 * (C[i] * y[i])\n    return wGradient, bGradient\n\n\n#prediction method for single model\n# X - samples to be tested on (usually testing samples)\n# w - weights of the model\n# b - bias of the model\n#returns a list of predictions for the given model on X\ndef predict(X, w, b):\n    predictions = w @ X.T + b\n    predictions = np.sign(predictions)\n    return predictions\n\n\n#scoring method for some given predictions\n# predictions - list of predictions made on X with some model\n# y - true labels of the samples\n#returns a float between 0.0 and 1.0 denoting the accuracy of the predictions\ndef score(predictions, y):\n    numCorrect = np.sum(predictions == y.T)\n    accuracy = numCorrect / y.shape[0]\n    return accuracy\n\n\n#polynomial transformation\n# X - input matrix to be transformed\n# g - gamma coefficient modifying the dot product\n# c - constant added to the dot product result\n# d - polynomial degree\n#returns transformed matrix X\ndef poly_kernel(X, g, c, d):\n    n = X.shape[0]\n    Xk = np.zeros((n, n))\n\n    for i in range(n):\n        for j in range(n):\n            Xk[i, j] = (g * X[i] @ X[j].T + c) ** d\n\n    return Xk\n\n\n#radial basis function transformation\n# X - input matrix to be transformed\n# g - gamma coefficient modifier, this value is multiplied by -1\n#returns transformed matrix X\ndef rbf_kernel(X, g):\n    n = X.shape[0]\n    Xk = np.zeros((n,n))\n\n    for i in range(n):\n        for j in range(n):\n            norm_sq = np.linalg.norm(X[i] - X[j]) ** 2\n            Xk[i, j] = np.exp(-g * norm_sq)\n    \n    return Xk\n\n\n#sigmoid kernel transformation\n# X - input matrix to be transformed\n# g - gamma coefficient modifier\n# c- constant added to dot product\n#returns transformed matrix X\ndef sigmoid_kernel(X, g, c):\n    n = X.shape[0]\n    Xk = np.zeros((n, n))\n\n    for i in range(n):\n        for j in range(n):\n            Xk[i, j] = np.tanh(g * (X[i] @ X[j]) + c)\n    \n    return Xk\n\n\n#ensemble learning - bagging\n# tX - training samples\n# tY - training labels\n# vX - validation samples\n# vY - validation labels\n# batchSize - hyperparameter\n# learningRate - hyperparameter\n# epochs - hyperparameter\n# numModels - dictates how many weak classifiers are trained\n# spp - samples per partition for each model; int\n#returns list of weights, list of biases, and list of losses\n#sanity; make sure numModels <= vX.size\ndef bagging(tX, tY, vX, vY, C, batchSize, learningRate, epochs, numModels, spp):\n    ws = []\n    bs = []\n    ls = []\n\n    tXs, tYs = bagging_split(tX, tY, numModels, spp)\n\n    for i in range(numModels):\n        w, b, lossList = fit(C, tXs[i], tYs[i], batchSize, learningRate, epochs)\n        ws.append(w)\n        bs.append(b)\n        ls.append(lossList)\n\n    return ws, bs, ls\n\n\n#prediction function for ensemble learning\n# X - testing samples\n# ws - list of weights; 2d\n# bs - list of biases; 2d\n# weighted - boolean variable; \n#    if true, accuracy of model will impact the weight of its vote\n# accs - accuracy of models\n# offset - float 0 <= offset < 1;\n#    advanced optimization, offsets model contribution to prediction by\n#    the set amount. 0.5 flips the prediction contribution of models with \n#    less than 50% accuracy\n#    this parameter also sharply reduces the contribution of models near the offset\n#    default = 0\n#returns the predictions for all input samples X\ndef ensemble_predict(X, ws, bs, weighted, accs, offset = 0.0):\n    predictions = np.zeros(X.shape[0])\n\n    if (weighted == True):\n        accs = np.array(accs)\n        accs -= offset\n        for i in range(len(ws)):\n            preds = ws[i] @ X.T + bs[i]\n            preds = np.float64(preds)\n            preds *= accs[i]\n            predictions += preds\n        \n        predictions = np.sign(predictions)\n    else:\n        for i in range(len(ws)):\n            preds = ws[i] @ X.T + bs[i]\n            preds = np.float64(preds)\n            preds = np.sign(preds)\n            predictions += preds\n        \n        predictions = np.sign(predictions)\n\n    return predictions\n\n\n#calculates a list of accuracies for multiple models\n#indeed use for bagging and boosting\n# X - testing samples\n# y - testing labels\n# ws - list of weights representing the models; 2d\n# bs - list of biases for the models\n#returns accuracy of all models\ndef get_bagging_acc(X, y, ws, bs):\n    accs = []\n\n    for i in range(len(ws)):\n        predictions = predict(X, ws[i], bs[i])\n        accs.append(score(predictions, y))\n\n    return accs\n\n\n#wrapper for ensemble learning prediction\n# X - test samples\n# y - test labels\n# ws - list of weights\n# bs - list of biases\n# weighted - boolean \n# offset - float\n# accs - model influence on prediction\n#returns accuracy of bagged models\ndef ensemble_wrapper(X, y, ws, bs, weighted, offset = 0.0, accs = 0):\n    if (accs == 0):\n        accs = get_bagging_acc(X, y, ws, bs)\n\n    predictions = ensemble_predict(X, ws, bs, weighted, accs, offset)\n    acc = score(predictions, y)\n\n    return acc\n\n\n#wrapper for single model predictions\n# X - test samples\n# y - test labels\n# w - weights\n# b - bias\n#returns accuracy of model\ndef wrapper(X, y, w, b):\n    predictions = predict(X, w, b)\n    acc = score(predictions, y)\n\n    return acc\n\n\n#boosting method\n# tX - training samples\n# tY - training labels\n# vX - validation samples\n# vY - validation labels\n# batchSize - hyperparameter\n# learningRate - hyperparameter\n# epochs - hyperparameter\n# numModels - number of models used in ensemble\n# spp - samples per parition\n#returns list of weights, list of biases, list of losses, and list of model contributions\ndef boosting(tX, tY, vX, vY, batchSize, learningRate, epochs, numModels, spp):\n    ws = []\n    bs = []\n    ls = []\n    sWs = np.ones(tX.shape[0])\n    sWs /= tX.shape[0]\n    lWs = []\n\n    for i in range(numModels):\n        tx, ty, ti = boosting_split(tX, tY, spp)\n        w, b, lossList = fit(1, tx, ty, batchSize, learningRate, epochs)\n        ws.append(w)\n        bs.append(b)\n        ls.append(lossList)\n\n        predictions = predict(tx, w, b)\n        ty = ty.flatten()\n        mscInd = np.where(predictions != ty.T)[0]\n        error = 0.0\n        for j in range(mscInd.shape[0]):\n            error +=  sWs[ti[mscInd[j]]]\n        \n        beta = np.log(((1 - error) / max(error, 1e-10)))\n        delta = np.exp(beta)\n\n        for j in range(mscInd.shape[0]):\n            sWs[ti[mscInd[j]]] = sWs[ti[mscInd[j]]] * delta\n\n        sWs /= np.sum(sWs)\n        lWs.append(beta)\n\n    return ws, bs, ls, lWs\n\n\n#simulates data corruption by changing feature values to 0\n# X - input samples to be corrupted\n# dcSamples - number of corrupted samples; float between 0.0 - 1.0\n# dcFeatures - number of corrupted features in corrupted samples; 0.0 - 0.99\n# uniform - boolean signifying if all corrupted samples have the same number of corrupted features\n#returns corrupted samples X\ndef applyCorruption(X, dcSamples, dcFeatures, uniform):\n    numCorS = int(dcSamples * X.shape[0])\n    numCorF = int(dcFeatures * X.shape[1])\n\n    if (uniform == True):\n        for i in range(numCorS):\n            for j in range(numCorF):\n                corInd = np.random.randint(X.shape[1])\n                X[i, corInd] = 0.0\n    else:\n        for i in range(numCorS):\n            numCorR = np.random.randint(numCorF)\n            for j in range(numCorR):\n                corInd = np.random.randint(X.shape[1])\n                X[i, corInd] = 0.0\n\n    return X\n\n\n#calculates the correlation coefficient between all features\n#formula utilized is \"pearson's r\"\n# X - input samples\n#returns a correlation matrix cor of all correlation coefficients\ndef calc_correlation(X):\n    mean = np.zeros(X.shape[1])\n    std = np.zeros(X.shape[1])\n    cov = np.zeros((X.shape[1], X.shape[1]))\n    cor = np.zeros((X.shape[1], X.shape[1]))\n\n    for i in range(X.shape[1]):\n        mean[i] = np.sum(X[:, i]) / X.shape[1]\n\n    for i in range(X.shape[1]):\n        std[i] = np.sqrt(np.sum((X[:, i] - mean[i]) ** 2) / X.shape[1])\n\n    for i in range(X.shape[1]):\n        for j in range(X.shape[1]):\n            c = 2\n            if (i == j):\n                c = 1\n            cov[i, j] = np.sum((X[:, i] - mean[i]) * (X[:, j] - mean[j])) / (c * X.shape[1])\n\n    for i in range(X.shape[1]):\n        for j in range(X.shape[1]):\n            cor[i, j] = cov[i, j] / (std[i] * std[j])\n\n    return cor\n\n\n#calculates coefficient factor between mean of features\n# X - input samples\n#returns a coefficient matrix coef\ndef calc_coefficient(X):\n    mean = np.zeros(X.shape[1])\n    coef = np.zeros((X.shape[1], X.shape[1]))\n\n    for i in range(X.shape[1]):\n        mean[i] = np.sum(X[:, i]) / X.shape[1]\n\n    for i in range(X.shape[1]):\n        for j in range(X.shape[1]):\n            coef[i, j] = mean[i] / mean[j]\n\n    return coef\n\n\n#data reconstruction on some samples X\n# X - input samples\n# t - correlation threshold before data reconstruction is skipped; 0.0 <= t <= 1.0\n#returns X with reconstructed samples based on parameters\ndef data_reconstruct(X, t):\n    cor = calc_correlation(X)\n    coef = calc_coefficient(X)\n\n    for i in range(X.shape[0]):\n        for j in range(X.shape[1]):\n            corIndex = -1\n            val = 0.0\n            corList = cor[j, :]\n\n            while (X[i, j] == 0.0):\n                corIndex = np.where(np.absolute(corList) == max(np.absolute(corList)))\n                val = X[i, corIndex]\n                \n                if (corList[corIndex] < t):\n                    break\n\n                if (val != 0.0):\n                    X[i, j] = X[i, corIndex] * coef[corIndex, j]\n                else:\n                    corList[corIndex] = 0.0\n\n    return X\n\n\nif __name__ == \"__main__\":\n    #data parsing and trimming\n    X, y = parseAlzheimers(\"handwriting_alzheimers.csv\")\n\n    #important note here that normalization comes before kernelization\n    #we can also apply normalization after kernelization or both\n    X = normalize(X)\n\n    #due to the data having a low sample count, depending on the distribution of the shuffle\n    #accuracy can be extremely poor\n    #while shuffling help negate this, we need another technique to increase sample count\n    X, y = shuffle(X, y)\n\n    #kernelization happens here, comment out to skip kernelization\n    X = poly_kernel(X, 1, 1, 2)\n    #X = rbf_kernel(X, 1)\n    #X = sigmoid_kernel(X, 1, 1)\n\n    #normalization can happen here after kernelization\n    X = normalize(X)\n\n    #data split ratios\n    train_x, train_y, val_x, val_y, test_x, test_y = splitData(X, y, 0.7, 0.15, 0.15)\n\n    #training procedures, use only 1\n    #ws, bs, ls = bagging(train_x, train_y, val_x, val_y, 1, 5, 0.001, 100, 10, int(train_x.shape[0] / 3))\n    ws, bs, ls, lWs = boosting(train_x, train_y, val_x, val_y, 5, 0.001, 100, 10, int(train_x.shape[0] / 3))\n    #w, b, lossList = fit(1, train_x, train_y, 10, 0.001, 100)\n\n    #data corruption simulation for robustness\n    test_x = applyCorruption(test_x, 0.1, 0.1, False)\n\n    #data reconstruction for corrupted data\n    test_x = data_reconstruct(test_x, 0.25)\n\n    #prediction and scoring, use corresponding methods\n    #ensemble predictions; set weight to False for bagging, True for boosting\n    acc = ensemble_wrapper(test_x, test_y, ws, bs, True, 0.5, lWs)\n\n    #single model predictions\n    #acc = wrapper(test_x, test_y, w, b)\n\n    print(acc)","block_group":"06bd30d357214248b8359c57f601f9d3","execution_count":1,"outputs":[{"name":"stdout","text":"0.8148148148148148\n","output_type":"stream"}]},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"5e41852d28134d92a68f4186175a02cf","deepnote_cell_type":"text-cell-p"},"source":"","block_group":"51aa8b77e03949cc8a697fd917635551"},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=96e6bd14-424c-411e-98e0-e7aeafdb7a8f' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote":{},"orig_nbformat":2,"deepnote_notebook_id":"be9abce2ab7a4cbab281eeeef0f2ca43","deepnote_persisted_session":{"createdAt":"2023-11-21T21:45:52.143Z"},"deepnote_execution_queue":[]}}