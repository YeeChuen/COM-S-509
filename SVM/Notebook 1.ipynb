{"cells":[{"cell_type":"code","metadata":{"source_hash":"b12685c3","execution_start":1695591724479,"execution_millis":3226,"deepnote_to_be_reexecuted":false,"cell_id":"2459083f9fc64ba99786430d6963e5a1","deepnote_cell_type":"code"},"source":"import csv\nimport numpy as np \nimport matplotlib.pyplot as plt\nimport os\nimport itertools\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot\nimport numpy.linalg \nimport numpy.random\nfrom sklearn.utils import shuffle\n\n\n\n\ndef parseData(filename):\n    csv_data = pd.read_csv(filename)\n    numpy_data = csv_data.values\n    rows, columns = numpy_data.shape\n    X = numpy_data[:, :columns - 1]\n    y = numpy_data[:, columns - 1:]\n    X = np.array(X)\n    y = np.array(y)\n    return X, y\n\n\ndef splitData(X, y, trainSplit, valSplit, testSplit):\n    trainStop = int(trainSplit * X.shape[0])\n    valStop = int((trainSplit + valSplit) * X.shape[0])\n    train_x = X[0:trainStop, :]\n    train_y = y[0:trainStop]\n    val_x = X[trainStop:valStop, :]\n    val_y = y[trainStop:valStop]\n    test_x = X[valStop:, :]\n    test_y = y[valStop:]\n    return train_x, train_y, val_x, val_y, test_x, test_y\n\n\ndef normalize(X):\n    rangeX = np.zeros(X.shape[1])\n    minX = np.zeros(X.shape[1])\n    normX = np.zeros(X.shape)\n\n    for i in range(X.shape[1]):\n        minX[i] = min(X[:, i])\n        rangeX[i] = max(X[:, i]) - minX[i]\n    for i in range(X.shape[0]):\n        for j in range(X.shape[1]):\n            normX[i][j] = (X[i][j] - minX[j]) / rangeX[j]\n\n    return normX\n\n\ndef hingeLoss(C, w, b, X, y):\n    reg_term = 0.5 * (w.T @ w)\n    losses = np.zeros(X.shape[0])\n\n    for i in range(X.shape[0]):\n        opt_term = y[i] * ((w.T @ X[i]) + b)\n        losses[i] = reg_term + C * max(0, 1 - opt_term)\n    \n    return losses\n\n\ndef fit(C, X, y, batchSize, learningRate, epochs):\n    w = np.zeros(X.shape[1])\n    b = 0\n    lossList = []\n\n    for i in range(epochs):\n        losses = hingeLoss(C, w, b, X, y)\n        lossList.append(losses)\n        for batch in range(0, X.shape[0], batchSize):\n            batchX = X[batch:batch+batchSize, :]\n            batchy = y[batch:batch+batchSize, :]\n            wGradient, bGradient = gradient(C, batchX, batchy, w, b)\n            w = w - learningRate * wGradient\n            b = b - learningRate * bGradient\n    return w, b, lossList\n\n\ndef gradient(C, X, y, w, b):\n    wGradient = 0\n    bGradient = 0\n\n    for i in range(X.shape[0]):\n        dist = y[i] * (w.T @ X[i] + b)\n\n        if (dist < 1):\n            wGradient += -1 * (C * y[i] * X[i])\n            bGradient += -1 * (C * y[i])\n    return wGradient, bGradient\n\n\ndef predict(X, w, b):\n    predictions = w @ X.T + b\n    predictions = np.sign(predictions)\n    return predictions\n\n\ndef score(predictions, y):\n    numCorrect = np.sum(predictions == y.T)\n    accuracy = numCorrect / y.shape[0]\n    return accuracy\n\n\nif __name__ == \"__main__\":\n    X, y = parseData(\"handwriting_alzheimers.csv\")\n    X = X[:, 1:]\n    y = np.where(y == \"P\", 1, y)\n    y = np.where(y == \"H\", -1, y)\n    X = normalize(X)\n    #Due to the data have a low sample count, depending on the distribution of the shuffle\n    #accuracy can be extremely poor\n    #While shuffling help negate this, we need another technique to increase sample count\n    X, y = shuffle(X, y)\n    X, y = shuffle(X, y)\n    X, y = shuffle(X, y)\n    \n    train_x, train_y, val_x, val_y, test_x, test_y = splitData(X, y, 0.7, 0.15, 0.15)\n    w, b, lossList = fit(1, train_x, train_y, 10, 0.001, 100)\n    predictions = predict(test_x, w, b)\n    acc = score(predictions, test_y)\n    print(acc)\n\n    ","block_group":"06bd30d357214248b8359c57f601f9d3","execution_count":1,"outputs":[{"name":"stdout","text":"0.7777777777777778\n","output_type":"stream"}]},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"5e41852d28134d92a68f4186175a02cf","deepnote_cell_type":"text-cell-p"},"source":"","block_group":"51aa8b77e03949cc8a697fd917635551"},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=96e6bd14-424c-411e-98e0-e7aeafdb7a8f' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote":{},"orig_nbformat":2,"deepnote_notebook_id":"be9abce2ab7a4cbab281eeeef0f2ca43","deepnote_execution_queue":[]}}